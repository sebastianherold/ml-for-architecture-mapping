{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to easily reprocude the results of the paper \"A Comparison of Machine Learning-Based Text Classifiers for Mapping Source Code to Architectural Modules\" published at the \"Eighth Workshop on Software Architecture Erosion and Architectural Consistency (SAEroCon 2021)\".\n",
    "\n",
    "If you are running the notebook for the first time, you should edit the two last lines in the next cell:\n",
    "-Set STEP_PREPROCESS to True\n",
    "-Set STEP_EVALUATE to True\n",
    "\n",
    "These variables can be used to skip the computationally expensive steps of preprocessing and evaluation, respectively, in later runs and use previously computed results instead.\n",
    "\n",
    "Similarily, TEST_ALL_PRPROCESSING_SETTINGS can be used to (by setting it to false) to use only a couple of preprocessing settings from a configuration file instead of all the combinations possible (see paper for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports and scripts\n",
    "#import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pathmagic\n",
    "%matplotlib inline\n",
    "with pathmagic.context():\n",
    "    import Preprocess as Prep\n",
    "    import RelativePaths as RP\n",
    "    import Evaluation as Eva\n",
    "    import GatherData as Gather\n",
    "    import Graphs\n",
    "    import Utils\n",
    "    import Metrics\n",
    "#import ray\n",
    "#ray.shutdown()\n",
    "#ray.init()\n",
    "from IPython.display import display\n",
    "from itertools import chain, combinations\n",
    "import concurrent.futures\n",
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "systems = ['jabref', 'prom', 'team', 'ant', 'lucene']\n",
    "systemNames = {'jabref' : 'JabRef', 'prom' : 'ProM', 'team' : 'TeamMates', 'ant' : 'Ant', 'lucene' : 'Lucene'}\n",
    "#systems = ['jabref']\n",
    "#systemNames = {'jabref' : 'JabRef'}\n",
    "TEST_ALL_PREPROCESSING_SETTINGS = True\n",
    "\n",
    "STEP_PREPROCESS = False\n",
    "STEP_EVALUATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing settings\n",
    "path_to_yaml = '../config.yaml'\n",
    "config = Utils.read_yaml_file(path_to_yaml)\n",
    "files = {}\n",
    "\n",
    "for system in systems:\n",
    "    files[system] = config['file locations'][system]\n",
    "preprocess_settings = config['preprocess settings list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file_locations which will be relative to computer in use\n",
    "from pathlib import Path\n",
    "raw_data_csv = {}\n",
    "system_folder = {}\n",
    "tmp_csv = {}\n",
    "table_file = {}\n",
    "for system in systems:\n",
    "    raw_data_csv[system] = str(Path.cwd().parent / files[system]['raw data'])\n",
    "    system_folder[system] = str(Path.cwd().parent / files[system]['system folder'])\n",
    "    tmp_csv[system] = str(Path.cwd().parent / files[system]['tmp data'])\n",
    "    table_file[system] = str(Path.cwd().parent / files[system]['preprocess comparisons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the logging\n",
    "import logging\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "logfilename=f'../log/experiments.log'\n",
    "#formatter = logging.Formatter('[%(asctime)s] %(name)s %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(filename=logfilename)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(message)s',\n",
    "    handlers=[\n",
    "        file_handler\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "logger.info(\"Logfile created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the architectural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for system in systems:\n",
    "    Gather.gather_architectural_concerns_data(system_folder[system], raw_data_csv[system])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_df = {}\n",
    "for system in systems:\n",
    "    dataset_df[system] = pd.read_csv(raw_data_csv[system])\n",
    "    y_labels = dataset_df[system].Label.unique()\n",
    "    x_quantity = [len(dataset_df[system].loc[dataset_df[system]['Label']==label]) for label in y_labels]\n",
    "    tmp_df = pd.DataFrame({\n",
    "        'Labels' : y_labels,\n",
    "        'Quantity' : x_quantity\n",
    "    })\n",
    "    tmp_df = tmp_df.sort_values(by=['Quantity'])\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "    plt.barh(y=tmp_df.Labels, width=tmp_df.Quantity)\n",
    "    for i, v in enumerate(tmp_df.Quantity):\n",
    "        plt.text(v, i, str(v), color='black', fontweight='bold', ha='left', va='center')\n",
    "\n",
    "    plt.xlabel('Modules')\n",
    "    plt.ylabel('Number of files')\n",
    "    plt.title('Files per module for ' + systemNames[system])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of preprocessing settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_bin_array(i, dim):\n",
    "    result = np.zeros(dim[0]*dim[1])\n",
    "    j = -1\n",
    "    while i != 0:\n",
    "        bit = i % 2\n",
    "        result[j] = bit\n",
    "        j-=1\n",
    "        i = i // 2\n",
    "    return np.reshape(result, dim).astype(int)\n",
    "\n",
    "def add_default_preprocessing(settings):\n",
    "    for s in settings:\n",
    "        s.extend(['lc','sc','sw','jk', 'tow'])\n",
    "    return settings\n",
    "\n",
    "def create_setting(df):\n",
    "    if len(df.index) == 0:\n",
    "        return None\n",
    "    df_temp = df.replace(1, pd.Series(df.columns, df.columns))\n",
    "    df_temp.insert(0, 'step', df_temp.index)\n",
    "    result = [list(filter(lambda x: x != 0,l)) for l in df_temp.to_numpy().tolist()]\n",
    "    return add_default_preprocessing(result)\n",
    "  \n",
    "pp_steps = ['scw', 'stem']\n",
    "pp_parts = ['pac', 'lib', 'c', 'pm', 'com']\n",
    "pp_parts_powerset = list(chain.from_iterable(combinations(pp_parts, r) for r in range(1, len(pp_parts)+1)))\n",
    "available_settings = []\n",
    "if TEST_ALL_PREPROCESSING_SETTINGS:\n",
    "    for parts in pp_parts_powerset:\n",
    "        settings = [int_to_bin_array(i, (len(parts), len(pp_steps))) for i in range(2**(len(pp_steps)*len(parts)))]\n",
    "        available_settings.extend([create_setting(pd.DataFrame(s, index=parts, columns=pp_steps)) for s in settings])\n",
    "else:\n",
    "    for setting_id, setting in preprocess_settings.items():\n",
    "        available_settings.append(setting)\n",
    "available_settings = [(i,available_setting) for i, available_setting in enumerate(available_settings)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rank the available preprocessing settings for all classifiers per system by accuracy. Evaluation at a training set size of 0.1 and Monte Carlo CV with 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if STEP_PREPROCESS:\n",
    "    MAX_TASKS = 16\n",
    "\n",
    "    for system in systems:\n",
    "        print(\"Processing system \" + systemNames[system] + \"...\")\n",
    "        Prep.clear_preprocessing_cache()\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                settings_iterator = iter(available_settings)\n",
    "                pp_futures = [executor.submit(Prep.lazy_preprocess_settings,\n",
    "                                        setting,\n",
    "                                        raw_data_csv[system],\n",
    "                                        tmp_csv[system] + os.path.sep + str(setting_id) + \".csv\")\n",
    "                              for setting_id, setting in itertools.islice(settings_iterator, MAX_TASKS)]\n",
    "                while pp_futures:\n",
    "                    done, pp_futures = concurrent.futures.wait(pp_futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                    for setting_id, setting in itertools.islice(settings_iterator, len(done)):\n",
    "                        pp_futures.add(executor.submit(Prep.lazy_preprocess_settings,\n",
    "                                        setting,\n",
    "                                        raw_data_csv[system],\n",
    "                                        tmp_csv[system] + os.path.sep + str(setting_id) + \".csv\"))\n",
    "        except Exception as e:\n",
    "            logger.info(\"Error: {0}\".format(e))\n",
    "    print(\"Preprocessing complete.\")\n",
    "else:\n",
    "    print(\"Preprocessing deactivated - reusing previous results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [\n",
    "    'classifier',\n",
    "    'setting_id',\n",
    "    'settings', \n",
    "    'accuracy', \n",
    "    'w_avg_precision', \n",
    "    'w_avg_recall'\n",
    "]\n",
    "\n",
    "MAX_TASKS = 16\n",
    "WORKER_THREADS = 2\n",
    "table_lock  = threading.Lock()\n",
    "main_table = {}\n",
    "test_size=0.8\n",
    "fold_quantity = 10\n",
    "\n",
    "def evaluate_concurrently(system, setting_id, setting, test_size, n_splits):\n",
    "    if setting != None:\n",
    "        tmp_df = pd.read_csv(tmp_csv[system] + os.path.sep + str(setting_id) + \".csv\")\n",
    "        df_sliced = Utils.remove_concerns_under_quantity_threshold(tmp_df)\n",
    "        # Train and gather evaluation metrics\n",
    "        logging.getLogger().info(\"Training started for setting \" + str(setting_id))\n",
    "        evaluate = Eva.Evaluation(df_sliced, CountVectorizer(), test_size, n_splits, 10)\n",
    "        metrics = evaluate.evaluate_all()\n",
    "        table_lock.acquire()\n",
    "        for m in metrics:\n",
    "            row = Utils.make_dataframe_row(m, setting, \"s\"+str(setting_id))\n",
    "            main_table[system] = main_table[system].append(row, ignore_index=True)\n",
    "        table_lock.release()\n",
    "        logging.getLogger().info(\"Training done for setting \" + str(setting_id))\n",
    "\n",
    "if STEP_EVALUATE:\n",
    "    for system in systems:\n",
    "        print(\"Evaluating settings for system \" + systemNames[system] + \"...\")\n",
    "        main_table[system] = pd.DataFrame(columns=df_columns)\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=WORKER_THREADS) as executor:\n",
    "                settings_iterator = iter(available_settings)\n",
    "                eval_futures = [executor.submit(evaluate_concurrently,\n",
    "                                              system,\n",
    "                                              setting_id,\n",
    "                                              setting,\n",
    "                                              test_size,\n",
    "                                              fold_quantity)\n",
    "                              for setting_id, setting in itertools.islice(settings_iterator, MAX_TASKS)]\n",
    "                while eval_futures:\n",
    "                    done, eval_futures = concurrent.futures.wait(eval_futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                    for setting_id, setting in itertools.islice(settings_iterator, len(done)):\n",
    "                        eval_futures.add(executor.submit(evaluate_concurrently,\n",
    "                                            system,\n",
    "                                            setting_id,\n",
    "                                            setting,\n",
    "                                            test_size,\n",
    "                                            fold_quantity))\n",
    "                main_table[system].to_csv(table_file[system], index=False)\n",
    "        except Exception as e:\n",
    "            logger.info(\"Error: {0}\".format(e))\n",
    "    print(\"Evaluation of preprocessing settings complete.\")\n",
    "else:\n",
    "    print(\"Evaluation of preprocessing settings deactivated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for adding columns for code parts\n",
    "code_elems = {'package_decl' : \"'pac'\", 'imports' : \"'lib'\", 'class_decl' : \"'c'\", 'public_methods' : \"'pm'\", 'comments' : \"'com'\"}\n",
    "for system in systems:\n",
    "    main_table[system] = pd.read_csv(table_file[system])\n",
    "    for elem in code_elems.keys():\n",
    "        main_table[system][elem] = 0\n",
    "        main_table[system].loc[main_table[system].settings.str.contains(code_elems[elem])==True, elem] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results per system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_settings = ['s0', 's1', 's2', 's3']\n",
    "\n",
    "for system in systems:\n",
    "    mask = main_table[system]['setting_id'].isin(simple_settings)\n",
    "    main_table[system] = main_table[system][~mask]\n",
    "    main_table[system]['system'] = systemNames[system]\n",
    "    print(systemNames[system])\n",
    "    display(main_table[system].sort_values(by='accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results by classifier and setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat(main_table, axis=0, ignore_index=True)\n",
    "df_grouped = df_total.drop(labels=['system', 'settings', 'setting_id'], axis=1)\n",
    "df_grouped = df_grouped.groupby(by=['classifier', 'package_decl', 'imports', 'class_decl', 'public_methods', 'comments']).mean()\n",
    "df_grouped.sort_values(by='accuracy',ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = df_grouped.reset_index()\n",
    "df_h = df_h.set_index([c for c in code_elems.keys()]).sort_index()\n",
    "df_h = pd.pivot_table(df_h, values='accuracy', columns='classifier', index=code_elems.keys()).transpose()\n",
    "plt.figure(figsize=(12,2))\n",
    "ax = sns.heatmap(df_h, annot=True, cbar=False, annot_kws={'rotation': 90, 'fontsize' : 13})\n",
    "ax.set_xlabel(\"Package declaration - import statement - class declaration -\\npublic methods - comments extracted (0=no/1=yes)\", fontsize=14)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_yticklabels(['Log. Regression', 'Naive Bayes', 'SVM'], fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd = df_total.drop(labels=['system'], axis=1)\n",
    "df_sd = df_sd.groupby(by=['classifier', 'package_decl', 'imports', 'class_decl', 'public_methods', 'comments', 'settings', 'setting_id']).mean()\n",
    "df_sd = df_sd.reset_index().set_index([c for c in code_elems.keys()]).sort_index()\n",
    "df_sd = df_sd.drop(labels=['settings','setting_id'], axis=1)\n",
    "df_sd = df_sd.groupby(by=['classifier', 'package_decl', 'imports', 'class_decl', 'public_methods', 'comments']).std()\n",
    "df_sd = df_sd.reset_index().set_index([c for c in code_elems.keys()]).sort_index()\n",
    "\n",
    "df_sd = pd.pivot_table(df_sd, values='accuracy', columns='classifier', index=code_elems.keys()).transpose()\n",
    "plt.figure(figsize=(12,2))\n",
    "ax = sns.heatmap(df_sd, annot=True, cbar=False, fmt=\".3f\", annot_kws={'rotation': 90, 'fontsize' : 12})\n",
    "ax.set_xlabel(\"Package declaration - import statement - class declaration -\\npublic methods - comments extracted (0=no/1=yes)\", fontsize=14)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_yticklabels(['Log. Regression', 'Naive Bayes', 'SVM'], fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['Support Vector Machines', 'Logistic Regression', 'Naive Bayes']\n",
    "df_total = pd.concat(main_table, axis=0, ignore_index=True)\n",
    "df_grouped = df_total.drop(labels=['system'], axis=1)\n",
    "df_grouped = df_grouped.groupby(by=['setting_id', 'classifier', 'package_decl', 'imports', 'class_decl', 'public_methods', 'comments']).mean()\n",
    "\n",
    "for cl in classifiers:\n",
    "    df_cl = df_grouped.reset_index()\n",
    "    df_cl = df_cl[df_cl['classifier']==cl].sort_values(by='accuracy',ascending=[False])\n",
    "    print(\"Results for \" + cl)\n",
    "    display(df_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results per setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_total.drop(labels=['system', 'classifier', 'setting_id', 'settings'], axis=1)\n",
    "df_grouped = df_grouped.groupby(by=[c for c in code_elems.keys()]).mean()\n",
    "df_grouped.sort_values(by='accuracy',ascending=[False]).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_total.drop(labels=['system', 'classifier'], axis=1)\n",
    "df_grouped = df_grouped.groupby(by=['setting_id','settings']).mean()\n",
    "df_grouped.sort_values(by='accuracy',ascending=[False]).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_pkg = df_grouped[df_grouped['package_decl'] == 0]\n",
    "df_no_pkg.sort_values(by='accuracy', ascending=[False]).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#41: best with package declarations\n",
    "#643: best without\n",
    "setting = available_settings[41][1]\n",
    "print(setting)\n",
    "n_splits = 10\n",
    "df_overview_columns = ['system', 'classifier', 'accuracy',\n",
    "                       'precision macro', 'precision weighted',\n",
    "                       'recall macro', 'recall weighted',\n",
    "                       'f1 macro', 'f1 weighted']\n",
    "df_overview = pd.DataFrame(columns=df_overview_columns)\n",
    "\n",
    "for system in systems:\n",
    "    Prep.preprocess_settings(setting, raw_data_csv[system], tmp_csv[system] + os.path.sep + str(41) + \".csv\")\n",
    "    tmp_df = pd.read_csv(tmp_csv[system]  + os.path.sep + str(41) + \".csv\")\n",
    "    df_sliced = Utils.remove_concerns_under_quantity_threshold(tmp_df)\n",
    "\n",
    "    feature_representation = CountVectorizer()\n",
    "    results = {}\n",
    "    evaluate = Eva.Evaluation(df_sliced, feature_representation, test_size, n_splits, 10)\n",
    "    results[\"Logistic Regression\"] = Metrics.get_average_classification_report(evaluate.evaluate_MaxEnt())\n",
    "    results[\"Support Vector Machines\"] = Metrics.get_average_classification_report(evaluate.evaluate_SVM())\n",
    "    results[\"Naive Bayes\"] = Metrics.get_average_classification_report(evaluate.evaluate_Naive_Bayes())\n",
    "    for key, value in results.items():\n",
    "        item = {'system' : systemNames[system],\n",
    "                'classifier' : key,\n",
    "                'accuracy' : value.loc['accuracy'][0],\n",
    "                'precision macro' : value.loc['macro avg']['precision'],\n",
    "                'precision weighted' : value.loc['weighted avg']['precision'],\n",
    "                'recall macro' : value.loc['macro avg']['recall'],\n",
    "                'recall weighted' : value.loc['weighted avg']['recall'],\n",
    "                'f1 macro' : value.loc['macro avg']['f1-score'],\n",
    "                'f1 weighted' : value.loc['weighted avg']['f1-score']\n",
    "               }\n",
    "        df_overview = df_overview.append(item, ignore_index=True)\n",
    "df_overview\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl = df_overview.drop(labels={'system'}, axis=1)\n",
    "df_cl = df_cl.groupby(by=\"classifier\").mean()\n",
    "df_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(12,5))\n",
    "metrics = [\"accuracy\", \"precision weighted\", \"f1 weighted\"]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plot = sns.barplot(data=df_overview,\n",
    "                    y=\"system\",\n",
    "                    x=metric,\n",
    "                    palette=\"colorblind\",\n",
    "                    alpha=0.8,\n",
    "                    hue=\"classifier\",\n",
    "                    ax=axs[i]\n",
    "               )\n",
    "    for p in plot.patches:\n",
    "        plot.annotate(format(p.get_width(), '.3f'),\n",
    "                     (p.get_x() + p.get_width() - 0.01, p.get_y() + p.get_height()/2),\n",
    "                     ha = 'right', va = 'center',\n",
    "                     textcoords = 'offset points',\n",
    "                     xytext  = (0, -1),\n",
    "                     fontsize=10)\n",
    "for ax in axs:\n",
    "    ax.set(xlim=(0.4, 1.0))\n",
    "    ax.label_outer()\n",
    "    ax.get_legend().remove()\n",
    "handles, labels = axs[2].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.1, 0, 0), loc='lower center', ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of impact of relative training set size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At fixed preprocessing setting s0, we look at how training set size affects the performance of the classifiers. The selected relative sizes are stratified which means that each module is represented in the training set according to its size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting = preprocess_settings['s0']\n",
    "df_sliced = {}\n",
    "processed_data_csv = {}\n",
    "for system in systems:\n",
    "    processed_data_csv[system] = str(Path.cwd().parent / files[system]['data size percentage'])\n",
    "    Prep.preprocess_settings(setting, raw_data_csv[system], processed_data_csv[system])\n",
    "    processed_data_df = pd.read_csv(processed_data_csv[system])\n",
    "    df_sliced[system] = Utils.remove_concerns_under_quantity_threshold(processed_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sizes = [0.95, 0.9, 0.85, 0.8, 0.75]\n",
    "n_splits = 100\n",
    "maxEnt_reports = {}\n",
    "svm_reports = {}\n",
    "naive_reports = {}\n",
    "\n",
    "for system in systems:\n",
    "    print(\"Processing system \" + systemNames[system])\n",
    "    maxEnt_reports[system] = []\n",
    "    svm_reports[system] = []\n",
    "    naive_reports[system] = []\n",
    "    for test_size in test_sizes:\n",
    "        feature_representation = CountVectorizer()\n",
    "        # Train and gather evaluation metrics\n",
    "        evaluate = Eva.Evaluation(df_sliced[system], feature_representation, test_size, n_splits)\n",
    "        metrics_max_ent = evaluate.evaluate_MaxEnt()\n",
    "        metrics_svm = evaluate.evaluate_SVM()\n",
    "        metrics_naive = evaluate.evaluate_Naive_Bayes()\n",
    "        maxEnt_reports[system].append(Metrics.get_average_classification_report(metrics_max_ent))\n",
    "        svm_reports[system].append(Metrics.get_average_classification_report(metrics_svm))\n",
    "        naive_reports[system].append(Metrics.get_average_classification_report(metrics_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plot(title: str, x_axis, y_axis: dict, x_axis_name, y_axis_name, ax):\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylim(0.4, 1)\n",
    "    ax.set_ylabel(y_axis_name, fontsize=12)\n",
    "    ax.set_xlabel(x_axis_name, fontsize=10)\n",
    "\n",
    "    ax.scatter(x_axis, y_axis[\"maxEnt\"], s=40, c=\"r\", marker=\"o\", label=\"Log. Regr.\")\n",
    "    ax.plot(x_axis, y_axis[\"maxEnt\"], c=\"r\", linewidth=0.8)\n",
    "    \n",
    "    ax.scatter(x_axis, y_axis[\"naive\"], s=40, c=\"b\", marker=\"x\", label=\"Naive Bayes\")\n",
    "    ax.plot(x_axis, y_axis[\"naive\"], c=\"b\", linewidth=0.8)\n",
    "\n",
    "    ax.scatter(x_axis, y_axis[\"svm\"], s=40, c=\"y\", marker=\"^\", label=\"SVM\")\n",
    "    ax.plot(x_axis, y_axis[\"svm\"], c=\"y\", linewidth=0.8)\n",
    "\n",
    "    #plt.legend(loc=\"lower right\")\n",
    "    #plt.show()\n",
    " \n",
    "\n",
    "train_size = [str(format(1 - i, '.2f')) for i in test_sizes]\n",
    "fig, axs = plt.subplots(4, len(systems), figsize=(10, 6), sharex=True, sharey=True)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['accuracy'][0] for report in maxEnt_reports[system]],\n",
    "        'naive': [report.loc['accuracy'][0] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['accuracy'][0] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(systemNames[system], train_size, y_axis, \"\", \"Accuracy\" if i == 0 else \"\", axs[0,i])\n",
    "\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['weighted avg']['precision'] for report in maxEnt_reports[system]],\n",
    "        'naive': [report.loc['weighted avg']['precision'] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['weighted avg']['precision'] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(\"\", train_size, y_axis,\n",
    "                           \"\", \"W. avg. prec.\" if i == 0 else \"\", axs[1,i])\n",
    "\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['macro avg']['precision'] for report in maxEnt_reports[system]],\n",
    "        'naive': [report.loc['macro avg']['precision'] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['macro avg']['precision'] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(\"\", train_size, y_axis,\n",
    "                           \"\", \"Avg. prec.\" if i == 0 else \"\", axs[2,i])\n",
    "\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['macro avg']['recall'] for report in maxEnt_reports[system]],\n",
    "        'naive': [report.loc['macro avg']['recall'] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['macro avg']['recall'] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(\"\", train_size, y_axis,\n",
    "                           \"\", \"Avg. recall\" if i == 0 else \"\", axs[3,i])\n",
    "for ax in [a for b in axs for a in b]:\n",
    "    ax.label_outer()\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.05, 0, 0), loc='lower center', ncol=3, fontsize=14, markerscale=1.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of impact of absolute training set sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At fixed preprocessing s0, we look at the scenario that, for each module, k files can be used for training. This corresponds to asking a system expert to map k files for each module before we can automatically map. In general, we don't know how large modules are, hence this scenario is more realistic than the previous consideration of relative sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the preprocessing from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_df = {}\n",
    "for system in systems:\n",
    "    processed_data_csv[system] = str(Path.cwd().parent / files[system]['data size abs num'])\n",
    "    Prep.preprocess_settings(setting, raw_data_csv[system], processed_data_csv[system])\n",
    "    processed_data_df[system] = pd.read_csv(processed_data_csv[system])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_training_files = [3, 5, 10, 15, 20, 25]\n",
    "n_splits = 100\n",
    "maxEnt_reports = {}\n",
    "svm_reports = {}\n",
    "naive_reports = {}\n",
    "\n",
    "for system in systems:\n",
    "    maxEnt_reports[system] = []\n",
    "    svm_reports[system] = []\n",
    "    naive_reports[system] = []    \n",
    "    for training_files in nr_of_training_files:\n",
    "        feature_representation = CountVectorizer()\n",
    "        # Train and gather evaluation metrics\n",
    "        df_sliced = Utils.remove_concerns_under_quantity_threshold(processed_data_df[system], training_files)\n",
    "        evaluate = Eva.Evaluation(df_sliced, feature_representation, n_splits, numberOfFiles=training_files)\n",
    "        metrics_max_ent = evaluate.evaluate_MaxEnt(type = 'custom')\n",
    "        metrics_svm = evaluate.evaluate_SVM(type = 'custom')\n",
    "        metrics_naive = evaluate.evaluate_Naive_Bayes(type = 'custom')\n",
    "        maxEnt_reports[system].append(Metrics.get_average_classification_report(metrics_max_ent))\n",
    "        svm_reports[system].append(Metrics.get_average_classification_report(metrics_svm))\n",
    "        naive_reports[system].append(Metrics.get_average_classification_report(metrics_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [str(i) for i in nr_of_training_files]\n",
    "fig, axs = plt.subplots(4, len(systems), figsize=(10, 6), sharex=True, sharey=True)\n",
    "\n",
    "for i, system in enumerate(systems):\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['accuracy'][0] for report in maxEnt_reports[system]],        \n",
    "        'naive': [report.loc['accuracy'][0] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['accuracy'][0] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(systemNames[system], x_axis, y_axis, \"\", \"Accuracy\" if i == 0 else \"\", axs[0,i])\n",
    "\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['weighted avg']['precision'] for report in maxEnt_reports[system]],\n",
    "        'naive': [report.loc['weighted avg']['precision'] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['weighted avg']['precision'] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(systemNames[system], x_axis, y_axis,\n",
    "                           \"\", \"W. avg. prec.\" if i == 0 else \"\", axs[1,i])\n",
    "\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['macro avg']['precision'] for report in maxEnt_reports[system]],\n",
    "        'naive': [report.loc['macro avg']['precision'] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['macro avg']['precision'] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(systemNames[system], x_axis, y_axis,\n",
    "                           \"\", \"Avg. prec. \" if i == 0 else \"\", axs[2,i])\n",
    "\n",
    "\n",
    "    ##############RECALL#######################\n",
    "    y_axis = {\n",
    "        'maxEnt': [report.loc['macro avg']['recall'] for report in maxEnt_reports[system]],\n",
    "        'naive': [report.loc['macro avg']['recall'] for report in naive_reports[system]],\n",
    "        'svm': [report.loc['macro avg']['recall'] for report in svm_reports[system]]\n",
    "    }\n",
    "    line_plot(systemNames[system], x_axis, y_axis,\n",
    "                           \"\", \"Avg. recall\" if i == 0 else \"\", axs[3,i])\n",
    "for ax in [a for b in axs for a in b]:\n",
    "    ax.label_outer()\n",
    "handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.05, 0, 0), loc='lower center', ncol=3, fontsize=14, markerscale=1.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
